{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-17 13:48:10.526649: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-17 13:48:10.541692: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744894090.561589   80286 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744894090.566383   80286 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-17 13:48:10.585224: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  7624\n",
      "Number of training labels:  7624\n",
      "image shape after normalization:  (265, 224, 224, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/said/anaconda3/envs/cpu_env/lib/python3.10/site-packages/keras/src/layers/core/input_layer.py:25: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1744894093.203085   80286 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4269 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 4050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7624 validated image filenames.\n",
      "Found 265 validated image filenames.\n",
      "Found 265 validated image filenames.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "\n",
    "def extract_labels_from_directories(filepaths):\n",
    "    # Extract labels from directory names (assuming directory names are the class labels)\n",
    "    return [os.path.basename(os.path.dirname(filepath)) for filepath in filepaths]\n",
    "\n",
    "# Load image file paths\n",
    "train_image_paths = [os.path.join(root, fname) \n",
    "                     for root, _, files in os.walk(os.path.join(\"dataset\", \"train\")) \n",
    "                     for fname in files if fname.endswith('.jpg')]\n",
    "test_image_paths = [os.path.join(root, fname) \n",
    "                    for root, _, files in os.walk(os.path.join(\"dataset\", \"test\")) \n",
    "                    for fname in files if fname.endswith('.jpg')]\n",
    "valid_image_paths = [os.path.join(root, fname) \n",
    "                     for root, _, files in os.walk(os.path.join(\"dataset\", \"valid\")) \n",
    "                     for fname in files if fname.endswith('.jpg')]\n",
    "\n",
    "print(\"Number of training images: \", len(train_image_paths))\n",
    "# Extract labels from directory names\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels as integers\n",
    "label_encoder = LabelEncoder()\n",
    "train_labels = label_encoder.fit_transform(extract_labels_from_directories(train_image_paths))\n",
    "test_labels = label_encoder.transform(extract_labels_from_directories(test_image_paths))\n",
    "valid_labels = label_encoder.transform(extract_labels_from_directories(valid_image_paths))\n",
    "\n",
    "print(\"Number of training labels: \", len(train_labels))\n",
    "# Load images into numpy arrays\n",
    "# train_images = np.array([plt.imread(path) for path in train_image_paths])\n",
    "test_images = np.array([plt.imread(path) for path in test_image_paths])\n",
    "# valid_images = np.array([plt.imread(path) for path in valid_image_paths])\n",
    "\n",
    "# print(\"image shape: \", train_images.shape)\n",
    "\n",
    "# Normalize the input image so that each pixel value is between 0 to 1.\n",
    "# train_images = train_images.astype(np.float32) / 255.0\n",
    "test_images = test_images.astype(np.float32) / 255.0\n",
    "\n",
    "# Apply data augmentation to the training images\n",
    "# data_augmentation = tf.keras.Sequential([\n",
    "#     tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "#     tf.keras.layers.RandomRotation(0.2),\n",
    "#     tf.keras.layers.RandomZoom(0.2),\n",
    "#     tf.keras.layers.RandomContrast(0.2)\n",
    "# ])\n",
    "\n",
    "# # Augment the training images\n",
    "# train_images = np.array([data_augmentation(image[np.newaxis, ...])[0].numpy() for image in train_images])\n",
    "\n",
    "# print(\"image shape after normalization: \", train_images.shape)\n",
    "print(\"image shape after normalization: \", test_images.shape)\n",
    "# print(\"image shape after normalization: \", valid_images.shape)\n",
    "# print(\"labels shape: \", train_labels.shape)\n",
    "# print(\"labels shape: \", test_labels.shape)\n",
    "# print(\"labels shape: \", valid_labels.shape)\n",
    "\n",
    "regularizer = tf.keras.regularizers.l2(0.01)\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Conv2D(filters=128, kernel_size=(3, 3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.25), \n",
    "    tf.keras.layers.Dense(53, kernel_regularizer=regularizer) \n",
    "])\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1.0/255.0,\n",
    "    # rotation_range=10,  # Rotate images up to 10 degrees\n",
    "    # width_shift_range=0.1,  # Shift images horizontally by 10% of the width\n",
    "    # height_shift_range=0.1,  # Shift images vertically by 10% of the height\n",
    "    # shear_range=0.1,  # Shear transformations\n",
    "    # zoom_range=0.1,  # Zoom in/out by 10%\n",
    "    horizontal_flip=True,  # Flip images horizontally\n",
    "    vertical_flip=True,  # Flip images vertically\n",
    "    fill_mode='nearest'  # Fill in missing pixels after transformations\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'filename': train_image_paths, 'class': train_labels}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "valid_generator = valid_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'filename': valid_image_paths, 'class': valid_labels}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=pd.DataFrame({'filename': test_image_paths, 'class': test_labels}),\n",
    "    x_col='filename',\n",
    "    y_col='class',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='raw',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "                            loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "                                    from_logits=True),\n",
    "                            metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model...\n",
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/said/anaconda3/envs/cpu_env/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744894094.875989   80542 service.cc:148] XLA service 0x7538bc003ee0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1744894094.876028   80542 service.cc:156]   StreamExecutor device (0): NVIDIA GeForce RTX 4050 Laptop GPU, Compute Capability 8.9\n",
      "2025-04-17 13:48:14.901738: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "I0000 00:00:1744894095.019257   80542 cuda_dnn.cc:529] Loaded cuDNN version 90300\n",
      "2025-04-17 13:48:15.677718: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_921', 240 bytes spill stores, 240 bytes spill loads\n",
      "\n",
      "2025-04-17 13:48:15.948939: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1221', 376 bytes spill stores, 376 bytes spill loads\n",
      "\n",
      "I0000 00:00:1744894100.412162   80542 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "2025-04-17 13:48:26.179859: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_921', 64 bytes spill stores, 64 bytes spill loads\n",
      "\n",
      "2025-04-17 13:48:26.395534: I external/local_xla/xla/stream_executor/cuda/cuda_asm_compiler.cc:397] ptxas warning : Registers are spilled to local memory in function 'gemm_fusion_dot_1221', 628 bytes spill stores, 628 bytes spill loads\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239/239 - 25s - 105ms/step - accuracy: 0.2385 - loss: 3.3534 - val_accuracy: 0.4868 - val_loss: 1.8638\n",
      "Epoch 2/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.4811 - loss: 2.1255 - val_accuracy: 0.6679 - val_loss: 1.3414\n",
      "Epoch 3/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.6142 - loss: 1.6132 - val_accuracy: 0.7736 - val_loss: 1.0396\n",
      "Epoch 4/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.6889 - loss: 1.2904 - val_accuracy: 0.8000 - val_loss: 0.8736\n",
      "Epoch 5/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.7516 - loss: 1.0562 - val_accuracy: 0.8264 - val_loss: 0.8373\n",
      "Epoch 6/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.7882 - loss: 0.9182 - val_accuracy: 0.8415 - val_loss: 0.7295\n",
      "Epoch 7/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.8219 - loss: 0.7685 - val_accuracy: 0.8302 - val_loss: 0.8224\n",
      "Epoch 8/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.8480 - loss: 0.6597 - val_accuracy: 0.8302 - val_loss: 0.7430\n",
      "Epoch 9/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.8712 - loss: 0.5865 - val_accuracy: 0.8604 - val_loss: 0.6817\n",
      "Epoch 10/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.8834 - loss: 0.5401 - val_accuracy: 0.8302 - val_loss: 0.7702\n",
      "Epoch 11/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9002 - loss: 0.4785 - val_accuracy: 0.8528 - val_loss: 0.7180\n",
      "Epoch 12/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9057 - loss: 0.4585 - val_accuracy: 0.8604 - val_loss: 0.7073\n",
      "Epoch 13/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9213 - loss: 0.3953 - val_accuracy: 0.8528 - val_loss: 0.7498\n",
      "Epoch 14/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9267 - loss: 0.3633 - val_accuracy: 0.8642 - val_loss: 0.7123\n",
      "Epoch 15/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9356 - loss: 0.3461 - val_accuracy: 0.8679 - val_loss: 0.6610\n",
      "Epoch 16/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9397 - loss: 0.3221 - val_accuracy: 0.8717 - val_loss: 0.6738\n",
      "Epoch 17/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9461 - loss: 0.3028 - val_accuracy: 0.8566 - val_loss: 0.6595\n",
      "Epoch 18/40\n",
      "239/239 - 13s - 52ms/step - accuracy: 0.9425 - loss: 0.3105 - val_accuracy: 0.8642 - val_loss: 0.6960\n",
      "Epoch 19/40\n",
      "239/239 - 13s - 54ms/step - accuracy: 0.9490 - loss: 0.2808 - val_accuracy: 0.8453 - val_loss: 0.8159\n",
      "Epoch 20/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9513 - loss: 0.2769 - val_accuracy: 0.8717 - val_loss: 0.7072\n",
      "Epoch 21/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9637 - loss: 0.2397 - val_accuracy: 0.8679 - val_loss: 0.7672\n",
      "Epoch 22/40\n",
      "239/239 - 13s - 52ms/step - accuracy: 0.9578 - loss: 0.2388 - val_accuracy: 0.8566 - val_loss: 0.8577\n",
      "Epoch 23/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9603 - loss: 0.2339 - val_accuracy: 0.8830 - val_loss: 0.7193\n",
      "Epoch 24/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9608 - loss: 0.2465 - val_accuracy: 0.8642 - val_loss: 0.8085\n",
      "Epoch 25/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9604 - loss: 0.2259 - val_accuracy: 0.8528 - val_loss: 0.7726\n",
      "Epoch 26/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9655 - loss: 0.2259 - val_accuracy: 0.8755 - val_loss: 0.6960\n",
      "Epoch 27/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9633 - loss: 0.2211 - val_accuracy: 0.8642 - val_loss: 0.7836\n",
      "Epoch 28/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9697 - loss: 0.1994 - val_accuracy: 0.8528 - val_loss: 0.7907\n",
      "Epoch 29/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9655 - loss: 0.2032 - val_accuracy: 0.8340 - val_loss: 0.9465\n",
      "Epoch 30/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9645 - loss: 0.2039 - val_accuracy: 0.8491 - val_loss: 0.8733\n",
      "Epoch 31/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9697 - loss: 0.1967 - val_accuracy: 0.8679 - val_loss: 0.8229\n",
      "Epoch 32/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9722 - loss: 0.1855 - val_accuracy: 0.8755 - val_loss: 0.8264\n",
      "Epoch 33/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9683 - loss: 0.2041 - val_accuracy: 0.8679 - val_loss: 0.9298\n",
      "Epoch 34/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9727 - loss: 0.1829 - val_accuracy: 0.8528 - val_loss: 0.9633\n",
      "Epoch 35/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9728 - loss: 0.1760 - val_accuracy: 0.8604 - val_loss: 0.9275\n",
      "Epoch 36/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9713 - loss: 0.1840 - val_accuracy: 0.8377 - val_loss: 0.7501\n",
      "Epoch 37/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9728 - loss: 0.1716 - val_accuracy: 0.8528 - val_loss: 0.8246\n",
      "Epoch 38/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9742 - loss: 0.1729 - val_accuracy: 0.8604 - val_loss: 0.8754\n",
      "Epoch 39/40\n",
      "239/239 - 12s - 51ms/step - accuracy: 0.9777 - loss: 0.1580 - val_accuracy: 0.8566 - val_loss: 0.9005\n",
      "Epoch 40/40\n",
      "239/239 - 12s - 52ms/step - accuracy: 0.9763 - loss: 0.1699 - val_accuracy: 0.8755 - val_loss: 0.7572\n",
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.8556 - loss: 0.9130\n",
      "Test loss 0.8589, accuracy 84.91%\n"
     ]
    }
   ],
   "source": [
    "print(\"Training the model...\")\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=valid_generator,\n",
    "    epochs=40,\n",
    "    verbose=2,\n",
    ")\n",
    "\n",
    "score = model.evaluate(test_images, test_labels, verbose=1)\n",
    "print(\"Test loss {:.4f}, accuracy {:.2f}%\".format(score[0], score[1] * 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkv4gl5a2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpkv4gl5a2/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpkv4gl5a2'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 53), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  128892149771856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892149775376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150108176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150110112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150111168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150112048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150112576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150113456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150113984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  128892150114864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/said/anaconda3/envs/cpu_env/lib/python3.10/site-packages/tensorflow/lite/python/convert.py:997: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1744894600.450965   80286 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1744894600.450981   80286 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "2025-04-17 13:56:40.451300: I tensorflow/cc/saved_model/reader.cc:83] Reading SavedModel from: /tmp/tmpkv4gl5a2\n",
      "2025-04-17 13:56:40.451831: I tensorflow/cc/saved_model/reader.cc:52] Reading meta graph with tags { serve }\n",
      "2025-04-17 13:56:40.451846: I tensorflow/cc/saved_model/reader.cc:147] Reading SavedModel debug info (if present) from: /tmp/tmpkv4gl5a2\n",
      "I0000 00:00:1744894600.455832   80286 mlir_graph_optimization_pass.cc:401] MLIR V1 optimization pass is not enabled\n",
      "2025-04-17 13:56:40.456620: I tensorflow/cc/saved_model/loader.cc:236] Restoring SavedModel bundle.\n",
      "2025-04-17 13:56:40.531460: I tensorflow/cc/saved_model/loader.cc:220] Running initialization op on SavedModel bundle at path: /tmp/tmpkv4gl5a2\n",
      "2025-04-17 13:56:40.539358: I tensorflow/cc/saved_model/loader.cc:466] SavedModel load for tags { serve }; Status: success: OK. Took 88054 microseconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model post quantization size: 21754.92 KB\n",
      "Micro TFLite model has been quantized and saved to playing_card_model_micro.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n"
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:0'):\n",
    "    def representative_dataset():\n",
    "        for i, data in enumerate(train_generator):\n",
    "            if i >= 50:\n",
    "                break\n",
    "            # Use only the input data (X), not the labels (y)\n",
    "            yield [data[0][:1]]  # Take one sample at a time\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    # try to use unsigned int\n",
    "    converter.inference_input_type = tf.uint8  # Input type for OpenMV\n",
    "    converter.inference_output_type = tf.uint8  # Output type for OpenMV\n",
    "\n",
    "    tflite_micro_model = converter.convert()\n",
    "\n",
    "    tflite_micro_model_path = \"playing_card_model_micro.tflite\"\n",
    "    with open(tflite_micro_model_path, \"wb\") as f:\n",
    "        f.write(tflite_micro_model)\n",
    "\n",
    "    model_size = len(tflite_micro_model) / 1024  # Size in KB\n",
    "    print(f\"Model post quantization size: {model_size:.2f} KB\")\n",
    "\n",
    "print(f\"Micro TFLite model has been quantized and saved to {tflite_micro_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ace of clubs, ace of diamonds, ace of hearts, ace of spades, eight of clubs, eight of diamonds, eight of hearts, eight of spades, five of clubs, five of diamonds, five of hearts, five of spades, four of clubs, four of diamonds, four of hearts, four of spades, jack of clubs, jack of diamonds, jack of hearts, jack of spades, joker, king of clubs, king of diamonds, king of hearts, king of spades, nine of clubs, nine of diamonds, nine of hearts, nine of spades, queen of clubs, queen of diamonds, queen of hearts, queen of spades, seven of clubs, seven of diamonds, seven of hearts, seven of spades, six of clubs, six of diamonds, six of hearts, six of spades, ten of clubs, ten of diamonds, ten of hearts, ten of spades, three of clubs, three of diamonds, three of hearts, three of spades, two of clubs, two of diamonds, two of hearts, two of spades\n"
     ]
    }
   ],
   "source": [
    "folder_names = \", \".join(sorted(next(os.walk(\"dataset/train\"))[1]))\n",
    "print(folder_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
